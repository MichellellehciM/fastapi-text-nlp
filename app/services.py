import spacy
import numpy as np
import networkx as nx
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba
import jieba.analyse  # è®“ `TextRank` æ›´æº–ç¢º

#  åŠ è¼‰ NLP æ¨¡å‹
nlp = spacy.load("zh_core_web_sm")  # æ”¯æ´ä¸­æ–‡
nlp_en = spacy.load("en_core_web_sm")  # æ”¯æ´è‹±æ–‡

# è‡ªè¨‚ä¸­æ–‡åœç”¨è©
STOP_WORDS = {"çš„", "æ˜¯", "åœ¨", "å’Œ", "æœ‰", "å¯ä»¥", "é€²è¡Œ", "ä½¿ç”¨", "ä¾†", "èªª", "é€™å€‹", "ä½†", "ä¹Ÿ"}

def extract_keywords(text, num_keywords=5):
    """ NLP æå–é—œéµå­—ï¼ˆTextRank for ä¸­æ–‡, TF-IDF for è‹±æ–‡ï¼‰ """
    
    # å¦‚æœæ˜¯è‹±æ–‡ï¼Œä½¿ç”¨ TF-IDF ä¾†æå–é—œéµå­—
    if all(ord(c) < 127 for c in text):  # åˆ¤æ–·æ˜¯å¦ç‚ºç´”è‹±æ–‡
        tfidf = TfidfVectorizer(stop_words="english")
        vec = tfidf.fit_transform([text])
        keywords = np.array(tfidf.get_feature_names_out())[vec.toarray().argsort()[0, -num_keywords:]]
        return keywords.tolist()
    
    # å¦‚æœæ˜¯ä¸­æ–‡ï¼Œä½¿ç”¨ TextRank ä¾†æå–é—œéµå­—
    return jieba.analyse.textrank(text, topK=num_keywords, withWeight=False)


def summarize_text(text: str, num_sentences=None, num_keywords=5):
    """ NLP è‡ªå‹•æ‘˜è¦ (LexRank) + ç²¾æº–é—œéµå­—æå– (TextRank) """
    doc = nlp(text) if any(ord(c) > 127 for c in text) else nlp_en(text)
    sentences = [sent.text.strip() for sent in doc.sents]  # å»é™¤ç©ºç™½ & ç¢ºä¿å¥å­å®Œæ•´

    if not sentences:
        return "", []  # é¿å…è™•ç†ç©ºå…§å®¹æ™‚å ±éŒ¯

    # **æ ¹æ“šæ–‡ç« é•·åº¦å‹•æ…‹èª¿æ•´æ‘˜è¦é•·åº¦**
    if num_sentences is None:
        num_sentences = max(3, len(sentences) // 4)  # æ–‡ç« é•·åº¦è¶Šé•·ï¼Œæ‘˜è¦å¥æ•¸è¶Šå¤š

    # **ç”Ÿæˆæ‘˜è¦ (LexRankï¼Œé¡ä¼¼ TextRank ä½†æ›´é©åˆé•·æ–‡)**
    tfidf = TfidfVectorizer()
    sentence_vectors = tfidf.fit_transform(sentences).toarray()
    similarity_matrix = np.dot(sentence_vectors, sentence_vectors.T)

    # **ä½¿ç”¨ LexRank è¨ˆç®—å¥å­æ¬Šé‡**
    nx_graph = nx.from_numpy_array(similarity_matrix)
    scores = nx.pagerank(nx_graph)

    # **ä¾æ“šåˆ†æ•¸æ’åºä¸¦é¸æ“‡é‡è¦å¥å­**
    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)
    summary_sentences = [s for _, s in ranked_sentences[:num_sentences]]

    # **å°‡æ‘˜è¦è®Šæˆæ¢åˆ—å¼**
    summary = "\n- " + "\n- ".join(summary_sentences)

    # **æå–é—œéµå­— (ä½¿ç”¨ TextRank)**
    keywords = extract_keywords(text, num_keywords)

    return summary, keywords


# # æ¸¬è©¦
# text = "ä½ çš„æ–°èæ–‡ç« ..."
# summary = summarize_text(text)
# keywords = extract_keywords(text)
# print("ğŸ“Œ æ‘˜è¦ï¼š", summary)
# print("ğŸ”‘ é—œéµå­—ï¼š", keywords)